{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be532e36-787a-49ce-9fb0-66f29c359c45",
   "metadata": {},
   "source": [
    "# Hello Spark\n",
    "Demonstration based on the [Spark Quick Start](https://spark.apache.org/docs/latest/quick-start.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23802ec5-981f-49c8-8bea-54a45beaa1f3",
   "metadata": {},
   "source": [
    "# Create a Spark Session\n",
    "The SparkSession object is our connection to the Spark Context Manager running on the spark-master host.\n",
    "\n",
    "There are a few important details in the setting up of the SparkSession:\n",
    "1. The `appName` is what shows up in the \"Running Apps\" section of http://localhost:8080/ -- It'll move to \"Completed Apps\" once we call `.stop()` on this session.\n",
    "2. The `master` tells it where to our Spark config-manager so we can launch spark-applications from this session.\n",
    "3. The `spark.sql.warehouse.dir` tells it where to find our Hive tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96de8a0-0b9e-4a3b-bc83-75ac4d1ee0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd38c959-3f57-4456-83a1-e6e571ce1319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/17 12:48:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "    .appName(\"hello-pyspark\")\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.cores.max\", 2)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d2f19-11dc-4d6f-8d85-6dc23b6751ad",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "This is a very basic hello-world to make sure the we can run a little PySpark:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0230c4e-b8dc-4197-89e6-05b52cb1cef6",
   "metadata": {},
   "source": [
    "### Get Some Sample Data\n",
    "We pull Shakespeare's \"As You Like It\" from Project Gutenberg, and write it to `/opt/data`.  This is mounted to our `fileshare` volume which is mounted on this docker container as well as all of the spark-containers (master and worker(s)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a25e3f3-5e3c-44bc-8a22-ca97896da3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get('https://www.gutenberg.org/cache/epub/1121/pg1121.txt')\n",
    "with open('/opt/data/as-you-like-it.txt','w')as fp:\n",
    "    fp.write(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd8b419-3ea7-499b-8110-7e2acde23b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as-you-like-it.txt\n"
     ]
    }
   ],
   "source": [
    "ls /opt/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844aaaa-7434-41dc-afae-e830d74e9848",
   "metadata": {},
   "source": [
    "### Perform word-count on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510dc8d4-1653-4c61-99b8-913f3657a38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ayli = spark_session.read.text('/opt/data/as-you-like-it.txt')\n",
    "ans = ayli.count()\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cdac81d-9644-48d2-84d8-4884fbaae6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:37:43 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/04/16 21:37:43 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "ayli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e96b5-b57b-43e9-8e67-fbb06b7ceaa7",
   "metadata": {},
   "source": [
    "# Spark grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33b897d6-be9f-40ce-a4f7-b7d15bc732cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orlandos_lines = ayli.filter(ayli.value.contains(\"ORLANDO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce6a5101-c556-41cd-91d4-6bbaf56404c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|  ORLANDO,  \"   \"...|\n",
      "|Enter ORLANDO and...|\n",
      "|  ORLANDO. As I r...|\n",
      "|  ORLANDO. Go apa...|\n",
      "|  ORLANDO. Nothin...|\n",
      "|  ORLANDO. Marry,...|\n",
      "|  ORLANDO. Shall ...|\n",
      "|  ORLANDO. O, sir...|\n",
      "|  ORLANDO. Ay, be...|\n",
      "|  ORLANDO. Come, ...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orlandos_lines.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973ed6e-e9dd-48d3-8bed-f8cc8237cb67",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "426e9edd-b140-45a4-9d21-a10552c54e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "wordCounts = ayli.select(explode(split(ayli.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "_coll = wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6261ceb4-c443-4f2a-adfd-ff42fc131b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    4|\n",
      "|PERMISSION.|    7|\n",
      "|       some|   26|\n",
      "|  disgrace,|    1|\n",
      "|       hope|    8|\n",
      "|      still|    7|\n",
      "|         By|   24|\n",
      "| misplaced;|    1|\n",
      "|      those|    8|\n",
      "|    knight,|    1|\n",
      "| FREDERICK.|   20|\n",
      "|  wrestler?|    1|\n",
      "|    embrace|    1|\n",
      "|        art|   21|\n",
      "|      burs,|    1|\n",
      "| likelihood|    1|\n",
      "|     travel|    3|\n",
      "|assailants.|    1|\n",
      "|      cold,|    1|\n",
      "|    blossom|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796cdf1-e370-44d2-bbbd-75e0a2b9ecda",
   "metadata": {},
   "source": [
    "## Save as Parquet File\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39f6624c-c3dd-4c7d-9ce9-ecea5e4ef487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wordCounts.write.mode('overwrite').parquet('/opt/warehouse/wordcounts.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973833a6-4b07-4aff-af97-5ff6df419b7d",
   "metadata": {},
   "source": [
    "## Read back Parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b59f6a-6d4e-43c3-8abd-7a496defb0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|     online|    4|\n",
      "|PERMISSION.|    7|\n",
      "|       some|   26|\n",
      "|  disgrace,|    1|\n",
      "|       hope|    8|\n",
      "|      still|    7|\n",
      "|         By|   24|\n",
      "| misplaced;|    1|\n",
      "|      those|    8|\n",
      "|    knight,|    1|\n",
      "| FREDERICK.|   20|\n",
      "|  wrestler?|    1|\n",
      "|    embrace|    1|\n",
      "|        art|   21|\n",
      "|      burs,|    1|\n",
      "| likelihood|    1|\n",
      "|     travel|    3|\n",
      "|assailants.|    1|\n",
      "|      cold,|    1|\n",
      "|    blossom|    1|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wc2 = spark_session.read.parquet('/opt/warehouse/wordcounts.parquet/')\n",
    "wc2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5569fdc2-9ca2-4697-b489-f9c0f29e6b0f",
   "metadata": {},
   "source": [
    "### Enable SQL-querying\n",
    "Create a temp-view from wc2 with name \"wordcounts\" so we can reference that as a table name in subsequent SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbde442-e27f-4120-8a3d-2b646fe02fad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|   ROSALIND.|  201|\n",
      "|    ORLANDO.|  120|\n",
      "|      CELIA.|  109|\n",
      "|     Project|   78|\n",
      "| TOUCHSTONE.|   74|\n",
      "|       would|   68|\n",
      "|       shall|   61|\n",
      "|     JAQUES.|   57|\n",
      "|Gutenberg-tm|   53|\n",
      "|       Enter|   51|\n",
      "|       which|   50|\n",
      "|     OLIVER.|   37|\n",
      "|      should|   35|\n",
      "|       there|   35|\n",
      "|       these|   32|\n",
      "|     SENIOR.|   32|\n",
      "|       their|   31|\n",
      "|  electronic|   27|\n",
      "|      cannot|   27|\n",
      "|      Exeunt|   27|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:27:39 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/04/16 21:27:39 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "wc2.createOrReplaceTempView(\"wordcounts\")\n",
    "\n",
    "ans = spark_session.sql(\"SELECT * FROM wordcounts WHERE LEN(word) > 4 ORDER BY count DESC\")\n",
    "ans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56f92ac3-5ec9-455b-b9a7-6cfe45f498bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans.limit(10).write.json('/opt/warehouse/answer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a6ca5ed-f6c1-4cf6-973f-4b8fb8717d55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/16 21:00:50 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "23/04/16 21:00:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:978)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "list_of_dicts = ans.limit(10).rdd.map(lambda row: row.asDict()).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d476c5c-cb5f-48ea-89c2-3214150680e1",
   "metadata": {},
   "source": [
    "# Close Session\n",
    "This shuts down the executors running on the workers and relinquishes cluster resources associated with this app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5426c84e-d519-4e83-83fa-502cdaceb44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b59cf-52b4-4732-8e2f-3c950527780d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
